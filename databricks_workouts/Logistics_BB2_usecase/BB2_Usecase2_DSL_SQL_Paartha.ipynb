{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4e9db29-71e6-4ee6-a139-a2e6adafec63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Enterprise Fleet Analytics Pipeline: Focuses on the business outcome (analytics) and the domain (fleet/logistics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60102b53-da15-4c74-a306-b675d15c78d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![logistics](logistics_project.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c798b032-39ac-4bf6-9992-78159146fb4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Download the data from the below gdrive and upload into the catalog\n",
    "https://drive.google.com/drive/folders/1J3AVJIPLP7CzT15yJIpSiWXshu1iLXKn?usp=drive_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81ca9127-636a-4be2-ad66-6af734d83aa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##**1. Data Munging** -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73641445-2b65-4138-89f3-44e10449c278",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####1. Visibily/Manually opening the file and capture couple of data patterns (Manual Exploratory Data Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c46583e7-2c19-4286-89b5-105c80b79e7f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 6"
    }
   },
   "outputs": [],
   "source": [
    "rawdf1 = spark.read.csv(\"/Volumes/logistics_project/default/logistics/logistics_source1\") \n",
    "rawdf2 = spark.read.csv(\"/Volumes/logistics_project/default/logistics/logistics_source2\")\n",
    "rawdf1.show(2)\n",
    "rawdf2.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "507bfd1d-8d3d-417b-8da8-074150a9eec3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####2. Programatically try to find couple of data patterns applying below EDA (File: logistics_source1)\n",
    "1. Apply inferSchema and toDF to create a DF and analyse the actual data.\n",
    "2. Analyse the schema, datatypes, columns etc.,\n",
    "3. Analyse the duplicate records count and summary of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07056a65-bf95-4d9f-84be-269e7edd8da3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1.Apply inferSchema and toDF to create a DF and analyse the actual data.\n",
    "rawdf1 = spark.read.csv(\"/Volumes/logistics_project/default/logistics/logistics_source1\", inferSchema=True, header=True)\n",
    "rawdf2 = spark.read.csv(\"/Volumes/logistics_project/default/logistics/logistics_source2\", inferSchema=True, header=True)\n",
    "\n",
    "df1 = rawdf1.toDF(\"shipment_id\", \"first_name\", \"last_name\", \"age\", \"role\")\n",
    "df1.show(2)\n",
    "df2 = rawdf2.toDF(\"shipment_id\", \"first_name\", \"last_name\", \"age\", \"role\", \"hub_location\", \"vehicle_type\")\n",
    "df2.show(2)\n",
    "\n",
    "#2.Analyse the schema, datatypes, columns etc.,\n",
    "df1.printSchema()\n",
    "\n",
    "#3.Analyse the duplicate records count and summary of the dataframe.\n",
    "total_count = df1.count()\n",
    "distinct_count = df1.distinct().count()\n",
    "\n",
    "print(\"Total records    :\", total_count)\n",
    "print(\"Distinct records :\", distinct_count)\n",
    "print(\"Duplicate records:\", total_count - distinct_count)\n",
    "df1.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de27b31f-de9a-457b-be06-00ada4960419",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###a. Passive Data Munging -  (File: logistics_source1  and logistics_source2)\n",
    "Without modifying the data, identify:<br>\n",
    "Shipment IDs that appear in both master_v1 and master_v2<br>\n",
    "Records where:<br>\n",
    "1. shipment_id is non-numeric\n",
    "2. age is not an integer<br>\n",
    "\n",
    "Count rows having:\n",
    "3. fewer columns than expected\n",
    "4. more columns than expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "368a8248-1eaa-46b7-8631-ea8f0ad43691",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768661227652}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      },
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768661227674}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Shipment IDs appearing in both master_v1 and master_v2\n",
    "from pyspark.sql.functions import col\n",
    "common_shipments = (df1.select(col(\"shipment_id\").cast(\"string\")).intersect(df2.select(col(\"shipment_id\").cast(\"string\"))))\n",
    "#display(common_shipments)\n",
    "\n",
    "#1.shipment_id is non-numeric\n",
    "non_numeric_shipment_df1 = df1.filter(col(\"shipment_id\").rlike(\"^[0-9]+$\"))\n",
    "non_numeric_shipment_df2 = df2.filter(col(\"shipment_id\").rlike(\"^[0-9]+$\"))\n",
    "#display(non_numeric_shipment_df1)\n",
    "#display(non_numeric_shipment_df2)\n",
    "\n",
    "#2.age is not an integer\n",
    "non_integer_age_df1 = df1.filter(col(\"age\").rlike(\"^[0-9]+$\"))\n",
    "non_integer_age_df2 = df2.filter(col(\"age\").rlike(\"^[0-9]+$\"))\n",
    "display(non_integer_age_df1)\n",
    "display(non_integer_age_df2)\n",
    "\n",
    "#3.fewer columns than expected \n",
    "#4. more columns than expected\n",
    "\n",
    "print(df1.columns)\n",
    "print(df2.columns)\n",
    "\n",
    "Expected_cols_df1 =5\n",
    "Expected_cols_df2 =7\n",
    "\n",
    "from pyspark.sql.functions import size, split,lit\n",
    "# Read raw text to analyze column structure\n",
    "raw1_text = spark.read.text(\"/Volumes/logistics_project/default/logistics/logistics_source1\")\n",
    "raw2_text = spark.read.text(\"/Volumes/logistics_project/default/logistics/logistics_source2\")\n",
    "\n",
    "raw1_cols = raw1_text.withColumn(\"col_count\", size(split(col(\"value\"), \",\")))\n",
    "raw2_cols = raw2_text.withColumn(\"col_count\", size(split(col(\"value\"), \",\")))\n",
    "\n",
    "# Counts\n",
    "fewer_cols_df1 = raw1_cols.filter(col(\"col_count\") < Expected_cols_df1).count()\n",
    "more_cols_df1  = raw1_cols.filter(col(\"col_count\") > Expected_cols_df1).count()\n",
    "\n",
    "fewer_cols_df2 = raw2_cols.filter(col(\"col_count\") < Expected_cols_df2).count()\n",
    "more_cols_df2  = raw2_cols.filter(col(\"col_count\") > Expected_cols_df2).count()\n",
    "\n",
    "print(\"DF1 → fewer columns:\", fewer_cols_df1, \"| more columns:\", more_cols_df1)\n",
    "print(\"DF2 → fewer columns:\", fewer_cols_df2, \"| more columns:\", more_cols_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13f97700-cae1-4ab6-97e8-502ffbd1a6d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create a Spark Session Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c81fc736-6e9f-4093-9f40-8e047989b602",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###**b. Active Data Munging** File: logistics_source1 and logistics_source2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7b773a5-b1db-4b1f-bf3e-93067d0483ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####1.Combining Data + Schema Merging (Structuring)\n",
    "1. Read both files without enforcing schema\n",
    "2. Align them into a single canonical schema: shipment_id,\n",
    "first_name,\n",
    "last_name,\n",
    "age,\n",
    "role,\n",
    "hub_location,\n",
    "vehicle_type,\n",
    "data_source\n",
    "3. Add data_source column with values as: system1, system2 in the respective dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "698448be-00d8-41e3-a946-5809660a7d78",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 14"
    }
   },
   "outputs": [],
   "source": [
    "#1.Read both files without enforcing schema\n",
    "rawdf1= spark.read.csv(\"/Volumes/logistics_project/default/logistics/logistics_source1\",header=True,inferSchema=False)\n",
    "rawdf2= spark.read.csv(\"/Volumes/logistics_project/default/logistics/logistics_source2\",header=True,inferSchema=False)\n",
    "rawdf1.show()\n",
    "rawdf2.show()\n",
    "\n",
    "#2.Align them into a single canonical schema: shipment_id, first_name, last_name, age, role, hub_location, vehicle_type, data_source\n",
    "\n",
    "struct1 = \"shipment_id string, first_name string, last_name string, age string, role string\"\n",
    "struct2 = \"shipment_id string, first_name string, last_name string, age string, role string, hub_location string, vehicle_type string\"\n",
    "rawdf1= spark.read.schema(struct1).csv(\"/Volumes/logistics_project/default/logistics/logistics_source1\",header=True,inferSchema=False)\n",
    "rawdf2= spark.read.schema(struct2).csv(\"/Volumes/logistics_project/default/logistics/logistics_source2\",header=True,inferSchema=False)\n",
    "rawdf1.show()\n",
    "rawdf2.show()\n",
    "\n",
    "#3.Add data_source column with values as: system1, system2 in the respective dataframes\n",
    "rawdf1.withColumn(\"data_source\",lit(\"system1\")).show()\n",
    "rawdf2.withColumn(\"data_source\",lit(\"system2\")).show()\n",
    "\n",
    "rawdf_merged=rawdf1.unionByName(rawdf2,allowMissingColumns=True)\n",
    "display(rawdf_merged)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "628e4769-0e24-481b-8b5c-33204a91ed3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####2. Cleansing, Scrubbing: \n",
    "Cleansing (removal of unwanted datasets)<br>\n",
    "1. Mandatory Column Check - Drop any record where any of the following columns is NULL:shipment_id, role<br>\n",
    "2. Name Completeness Rule - Drop records where both of the following columns are NULL: first_name, last_name<br>\n",
    "3. Join Readiness Rule - Drop records where the join key is null: shipment_id<br>\n",
    "\n",
    "Scrubbing (convert raw to tidy)<br>\n",
    "4. Age Defaulting Rule - Fill NULL values in the age column with: -1<br>\n",
    "5. Vehicle Type Default Rule - Fill NULL values in the vehicle_type column with: UNKNOWN<br>\n",
    "6. Invalid Age Replacement - Replace the following values in age:\n",
    "\"ten\" to -1\n",
    "\"\" to -1<br>\n",
    "7. Vehicle Type Normalization - Replace inconsistent vehicle types: \n",
    "truck to LMV\n",
    "bike to TwoWheeler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b93bf85-e456-4868-ade4-4fb7147acdf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Cleansing (removal of unwanted datasets)\n",
    "\n",
    "#1.Mandatory Column Check - Drop any record where any of the following columns is NULL:shipment_id, role\n",
    "cleanseddf1=rawdf1.na.drop(how=\"any\",subset=[\"shipment_id\",\"role\"])\n",
    "cleanseddf2=rawdf2.na.drop(how=\"any\",subset=[\"shipment_id\",\"role\"])\n",
    "display(cleanseddf1)\n",
    "display(cleanseddf2)\n",
    "\n",
    "#2.Name Completeness Rule - Drop records where both of the following columns are NULL: first_name, last_name\n",
    "cleanseddf3=cleanseddf1.na.drop(how=\"any\",subset=[\"first_name\",\"last_name\"])\n",
    "cleanseddf4=cleanseddf2.na.drop(how=\"any\",subset=[\"first_name\",\"last_name\"])\n",
    "display(cleanseddf3)\n",
    "display(cleanseddf4)\n",
    "\n",
    "#3.Join Readiness Rule - Drop records where the join key is null: shipment_id\n",
    "joineddf=rawdf_merged.na.drop(how=\"any\",subset=[\"shipment_id\"])\n",
    "display(joineddf)\n",
    "\n",
    "#Scrubbing (convert raw to tidy)\n",
    "\n",
    "#4. Age Defaulting Rule - Fill NULL values in the age column with: -1\n",
    "scrubbeddf1 = joineddf.na.fill('-1',subset=[\"age\"])\n",
    "display(scrubbeddf1)\n",
    "\n",
    "\n",
    "#5.Vehicle Type Default Rule - Fill NULL values in the vehicle_type column with: UNKNOWN\n",
    "scrubbeddf2 = scrubbeddf1.na.fill('UNKNOWN',subset=[\"vehicle_type\"])\n",
    "display(scrubbeddf2) \n",
    "\n",
    "\n",
    "#6.Invalid Age Replacement - Replace the following values in age: \"ten\" to -1 \"\" to -1\n",
    "scrubbeddf3 = scrubbeddf2.replace(\"ten\", \"-1\", \"age\")\n",
    "display(scrubbeddf3)\n",
    "\n",
    "\n",
    "\n",
    "#7.Vehicle Type Normalization - Replace inconsistent vehicle types: truck to LMV bike to TwoWheeler\n",
    "fill_replace_value = {'Truck':'LMV','Bike':'TwoWheeler'}\n",
    "Normalized_df = scrubbeddf3.replace(fill_replace_value,\"vehicle_type\")\n",
    "display(Normalized_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b089e58-4b74-41e5-b050-bbfa8d249467",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####3. Standardization, De-Duplication and Replacement / Deletion of Data to make it in a usable format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccc60af7-9398-4de0-93ce-1f5bf9dd5c47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Creating shipments Details data Dataframe creation <br>\n",
    "1. Create a DF by Reading Data from logistics_shipment_detail.json\n",
    "2. As this data is a clean json data, it doesn't require any cleansing or scrubbing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6f72588-8e0f-4108-b48d-f4acba0423bb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 19"
    }
   },
   "outputs": [],
   "source": [
    "#1.Create a DF by Reading Data from logistics_shipment_detail.json\n",
    "\n",
    "shipment_df = spark.read.option(\"multiline\", \"true\").format(\"json\").load(\"/Volumes/logistics_project/default/logistics/logistics_shipment_detail_3000.json\")\n",
    "display(shipment_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cd9b438-99be-431d-a81a-493c23b2b998",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Standardizations:<br>\n",
    "\n",
    "1. Add a column<br> \n",
    "Source File: DF of logistics_shipment_detail_3000.json<br>: domain as 'Logistics',  current timestamp 'ingestion_timestamp' and 'False' as 'is_expedited'\n",
    "2. Column Uniformity: \n",
    "role - Convert to lowercase<br>\n",
    "Source File: DF of merged(logistics_source1 & logistics_source2)<br>\n",
    "vehicle_type - Convert values to UPPERCASE<br>\n",
    "Source Files: DF of logistics_shipment_detail_3000.json\n",
    "hub_location - Convert values to initcap case<br>\n",
    "Source Files: DF of merged(logistics_source1 & logistics_source2)<br>\n",
    "3. Format Standardization:<br>\n",
    "Source Files: DF of logistics_shipment_detail_3000.json<br>\n",
    "Convert shipment_date to yyyy-MM-dd<br>\n",
    "Ensure shipment_cost has 2 decimal precision<br>\n",
    "4. Data Type Standardization<br>\n",
    "Standardizing column data types to fix schema drift and enable mathematical operations.<br>\n",
    "Source File: DF of merged(logistics_source1 & logistics_source2) <br>\n",
    "age: Cast String to Integer<br>\n",
    "Source File: DF of logistics_shipment_detail_3000.json<br>\n",
    "shipment_weight_kg: Cast to Double<br>\n",
    "Source File: DF of logistics_shipment_detail_3000.json<br>\n",
    "is_expedited: Cast to Boolean<br>\n",
    "5. Naming Standardization <br>\n",
    "Source File: DF of merged(logistics_source1 & logistics_source2)<br>\n",
    "Rename: first_name to staff_first_name<br>\n",
    "Rename: last_name to staff_last_name<br>\n",
    "Rename: hub_location to origin_hub_city<br>\n",
    "6. Reordering columns logically in a better standard format:<br>\n",
    "Source File: DF of Data from all 3 files<br>\n",
    "shipment_id (Identifier), staff_first_name (Dimension)staff_last_name (Dimension), role (Dimension), origin_hub_city (Location), shipment_cost (Metric), ingestion_timestamp (Audit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a746f62-84f3-467b-a382-df553dc90a24",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Fix cast error for age column"
    }
   },
   "outputs": [],
   "source": [
    "# Define shipment_df before using it\n",
    "from pyspark.sql.functions import initcap,lower,upper,to_date,col,current_timestamp\n",
    "shipment_df = spark.read.option(\"multiline\", \"true\").format(\"json\").load(\"/Volumes/logistics_project/default/logistics/logistics_shipment_detail_3000.json\")\n",
    "\n",
    "#1.Add a column Source File: DF of logistics_shipment_detail_3000.json : domain as 'Logistics', current timestamp 'ingestion_timestamp' and 'False' as 'is_expedited'\n",
    "std_df = shipment_df.withColumn(\"domain\",lit(\"Logistics\")).withColumn(\"ingestion_timestamp\",current_timestamp()).withColumn(\"is_expedited\",lit(\"False\"))\n",
    "#display(std_df)\n",
    "\n",
    "#2.Column Uniformity: role - Convert to lowercase\n",
    "#Source File: DF of merged(logistics_source1 & logistics_source2)\n",
    "#vehicle_type - Convert values to UPPERCASE\n",
    "#Source Files: DF of logistics_shipment_detail_3000.json hub_location - Convert values to initcap case\n",
    "#Source Files: DF of merged(logistics_source1 & logistics_source2)\n",
    "\n",
    "\n",
    "std_csv_df = joineddf.withColumn(\"role\",lower(\"role\")).withColumn(\"hub_location\",initcap(\"hub_location\"))\n",
    "std_json_df = std_df.withColumn(\"vehicle_type\",upper(\"vehicle_type\"))\n",
    "#display(std_csv_df)\n",
    "#display(std_json_df)\n",
    "\n",
    "#3.Format Standardization:\n",
    "#Source Files: DF of logistics_shipment_detail_3000.json\n",
    "#Convert shipment_date to yyyy-MM-dd\n",
    "#Ensure shipment_cost has 2 decimal precision\n",
    "\n",
    "from pyspark.sql.functions import to_date, col, round\n",
    "\n",
    "std_format_json_df = std_json_df.withColumn(\"shipment_date\",to_date(col(\"shipment_date\"), \"dd-MM-yy\")).withColumn(\"shipment_cost\",round(col(\"shipment_cost\"), 2))\n",
    "#display(std_format_json_df)\n",
    "\n",
    "#4.Data Type Standardization\n",
    "#Standardizing column data types to fix schema drift and enable mathematical operations.\n",
    "#Source File: DF of merged(logistics_source1 & logistics_source2)\n",
    "#age: Cast String to Integer\n",
    "#Source File: DF of logistics_shipment_detail_3000.json\n",
    "#shipment_weight_kg: Cast to Double\n",
    "#Source File: DF of logistics_shipment_detail_3000.json\n",
    "#is_expedited: Cast to Boolean\n",
    "std_csv_df.printSchema()\n",
    "from pyspark.sql.functions import expr\n",
    "std_csv_df = std_csv_df.withColumn(\"age\",expr(\"try_cast(age as int)\"))\n",
    "std_csv_df.printSchema()\n",
    "\n",
    "std_format_json_df = std_format_json_df.withColumn(\"shipment_weight_kg\",col(\"shipment_weight_kg\").cast(\"double\")).withColumn(\"is_expedited\",col(\"is_expedited\").cast(\"boolean\"))\n",
    "std_format_json_df.printSchema()\n",
    "\n",
    "#5.Naming Standardization\n",
    "#Source File: DF of merged(logistics_source1 & logistics_source2)\n",
    "#Rename: first_name to staff_first_name\n",
    "#Rename: last_name to staff_last_name\n",
    "#Rename: hub_location to origin_hub_city\n",
    "\n",
    "std_csv_df = Normalized_df.withColumnRenamed(\"first_name\",\"staff_first_name\").withColumnRenamed(\"last_name\",\"staff_last_name\").withColumnRenamed(\"hub_location\",\"origin_hub_city\")\n",
    "#display(std_csv_df)\n",
    "\n",
    "#6.Reordering columns logically in a better standard format:\n",
    "#Source File: DF of Data from all 3 files\n",
    "#shipment_id (Identifier), staff_first_name (Dimension)staff_last_name (Dimension), role (Dimension), origin_hub_city (Location), #shipment_cost (Metric), ingestion_timestamp (Audit)\n",
    "\n",
    "\n",
    "#std_format_json_df = std_format_json_df.na.replace(\"shipment_id\", [\"id\"]).withColumn(\"shipment_id\", col(\"id\").cast(\"bigint\"))\n",
    "std_csv_df =std_csv_df.select(\"shipment_id\",\"staff_first_name\",\"staff_last_name\",\"role\",\"origin_hub_city\",\"age\",\"vehicle_type\")\n",
    "#std_format_json_df =std_format_json_df.select(\"shipment_cost\",\"ingestion_timestamp\")\n",
    "std_csv_df.show(2)\n",
    "std_format_json_df.show(2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bafa5c8-7355-4d4f-9caf-40e0e0d303f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Deduplication:\n",
    "1. Apply Record Level De-Duplication\n",
    "2. Apply Column Level De-Duplication (Primary Key Enforcement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fab6c118-712c-4785-8b0e-f517f17b2fdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1.Apply Record Level De-Duplication\n",
    "\n",
    "from pyspark.sql.functions import col, countDistinct, count, avg, sum, max, min, lit\n",
    "std_csv_df.printSchema()\n",
    "std_csv_df = std_csv_df.coalesce(1).dropDuplicates()\n",
    "\n",
    "std_format_json_df = std_format_json_df.coalesce(1).dropDuplicates()\n",
    "std_csv_df.show(2)\n",
    "std_format_json_df.show(2)\n",
    "\n",
    "#2.Apply Column Level De-Duplication (Primary Key Enforcement)\n",
    "\n",
    "std_csv_df = std_csv_df.dropDuplicates(['shipment_id'])\n",
    "std_format_json_df = std_format_json_df.dropDuplicates(['shipment_id']) \n",
    "std_csv_df.show(2)\n",
    "std_format_json_df.show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f34e7c5-3885-44b0-90b0-b189757d6845",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Self Join (Peer Finding) - Fixed"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "184ecd12-8081-4c8d-925e-d47c84e520d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##2. Data Enrichment - Detailing of data\n",
    "Makes your data rich and detailed <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "311d404f-b7d4-4e56-9f09-9367ec05e283",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### Adding of Columns (Data Enrichment)\n",
    "*Creating new derived attributes to enhance traceability and analytical capability.*\n",
    "\n",
    "**1. Add Audit Timestamp (`load_dt`)**\n",
    "Source File: DF of logistics_source1 and logistics_source2<br>\n",
    "* **Scenario:** We need to track exactly when this record was ingested into our Data Lakehouse for auditing purposes.\n",
    "* **Action:** Add a column `load_dt` using the function `current_timestamp()`.\n",
    "\n",
    "**2. Create Full Name (`full_name`)**\n",
    "Source File: DF of logistics_source1 and logistics_source2<br>\n",
    "* **Scenario:** The reporting dashboard requires a single field for the driver's name instead of separate columns.\n",
    "* **Action:** Create `full_name` by concatenating `first_name` and `last_name` with a space separator.\n",
    "* **Result:** \"Rajesh\" + \" \" + \"Kumar\" -> **\"Rajesh Kumar\"**\n",
    "\n",
    "**3. Define Route Segment (`route_segment`)**\n",
    "Source File: DF of logistics_shipment_detail_3000.json<br>\n",
    "* **Scenario:** The logistics team wants to analyze performance based on specific transport lanes (Source to Destination).\n",
    "* **Action:** Combine `source_city` and `destination_city` with a hyphen.\n",
    "* **Result:** \"Chennai\" + \"-\" + \"Pune\" -> **\"Chennai-Pune\"**\n",
    "\n",
    "**4. Generate Vehicle Identifier (`vehicle_identifier`)**\n",
    "Source File: DF of logistics_shipment_detail_3000.json<br>\n",
    "* **Scenario:** We need a unique tracking code that immediately tells us the vehicle type and the shipment ID.\n",
    "* **Action:** Combine `vehicle_type` and `shipment_id` to create a composite key.\n",
    "* **Result:** \"Truck\" + \"_\" + \"500001\" -> **\"Truck_500001\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c438a4af-5cee-43b6-a757-750611da8f36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1. Add Audit Timestamp (load_dt) Source File: DF of logistics_source1 and logistics_source2\n",
    "\n",
    "#Scenario: We need to track exactly when this record was ingested into our Data Lakehouse for auditing purposes.\n",
    "#Action: Add a column load_dt using the function current_timestamp().\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp, concat\n",
    "\n",
    "std_csv_df = std_csv_df.withColumn(\"load_dt\", current_timestamp())  \n",
    "std_csv_df.show(2)\n",
    "\n",
    "#2.Create Full Name (full_name) Source File: DF of logistics_source1 and logistics_source2\n",
    "#Scenario: The reporting dashboard requires a single field for the driver's name instead of separate columns.\n",
    "#Action: Create full_name by concatenating first_name and last_name with a space separator.\n",
    "#Result: \"Rajesh\" + \" \" + \"Kumar\" -> \"Rajesh Kumar\"\n",
    "\n",
    "std_csv_df = std_csv_df .withColumn(\"full_name\", concat(col(\"staff_first_name\"),lit(\" \"),col(\"staff_last_name\")))\n",
    "std_csv_df.show(10)\n",
    "std_csv_df.printSchema()\n",
    "\n",
    "#3.Define Route Segment (route_segment) Source File: DF of logistics_shipment_detail_3000.json\n",
    "\n",
    "#Scenario: The logistics team wants to analyze performance based on specific transport lanes (Source to Destination).\n",
    "##Action: Combine source_city and destination_city with a hyphen.\n",
    "#Result: \"Chennai\" + \"-\" + \"Pune\" -> \"Chennai-Pune\"\n",
    "\n",
    "std_format_json_df = std_format_json_df.withColumn(\"route_segment\",concat(col(\"source_city\"),lit(\"-\"),col(\"destination_city\")))\n",
    "std_format_json_df.show(10)\n",
    "\n",
    "#4. Generate Vehicle Identifier (vehicle_identifier) Source File: DF of logistics_shipment_detail_3000.json\n",
    "#Action: Combine vehicle_type and shipment_id to create a composite key.\n",
    "#Result: \"Truck\" + \"_\" + \"500001\" -> \"Truck_500001\"\n",
    "\n",
    "std_format_json_df = std_format_json_df.withColumn(\"vehicle_identifier\",concat(col(\"vehicle_type\"),lit(\"_\"),col(\"shipment_id\")))\n",
    "std_format_json_df.show(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e525645-18d1-4a16-9909-afc89a2ed57d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### Deriving of Columns (Time Intelligence)\n",
    "*Extracting temporal features from dates to enable period-based analysis and reporting.*<br>\n",
    "Source File: logistics_shipment_detail_3000.json<br>\n",
    "**1. Derive Shipment Year (`shipment_year`)**\n",
    "* **Scenario:** Management needs an annual performance report to compare growth year-over-year.\n",
    "* **Action:** Extract the year component from `shipment_date`.\n",
    "* **Result:** \"2024-04-23\" -> **2024**\n",
    "\n",
    "**2. Derive Shipment Month (`shipment_month`)**\n",
    "* **Scenario:** Analysts want to identify seasonal peaks (e.g., increased volume in December).\n",
    "* **Action:** Extract the month component from `shipment_date`.\n",
    "* **Result:** \"2024-04-23\" -> **4** (April)\n",
    "\n",
    "**3. Flag Weekend Operations (`is_weekend`)**\n",
    "* **Scenario:** The Operations team needs to track shipments handled during weekends to calculate overtime pay or analyze non-business day capacity.\n",
    "* **Action:** Flag as **'True'** if the `shipment_date` falls on a Saturday or Sunday.\n",
    "\n",
    "**4. Flag shipment status (`is_expedited`)**\n",
    "* **Scenario:** The Operations team needs to track shipments is IN_TRANSIT or DELIVERED.\n",
    "* **Action:** Flag as **'True'** if the `shipment_status` IN_TRANSIT or DELIVERED."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7a7d37c-cce6-4343-a514-1e797151705e",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768736737665}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1. 1. Derive Shipment Year (shipment_year)\n",
    "#Scenario: Management needs an annual performance report to compare growth year-over-year.\n",
    "#Action: Extract the year component from shipment_date.\n",
    "#Result: \"2024-04-23\" -> 2024\n",
    "from pyspark.sql.functions import *\n",
    "der_df = std_format_json_df.select(\"*\", substring(col(\"shipment_date\"),1,4).cast(\"int\").alias(\"shipment_year\"))\n",
    "#display(der_df)\n",
    "\n",
    "#2.Derive Shipment Month (shipment_month)\n",
    "#Scenario: Analysts want to identify seasonal peaks (e.g., increased volume in December).\n",
    "#Action: Extract the month component from shipment_date.\n",
    "#Result: \"2024-04-23\" -> 4 (April)\n",
    "\n",
    "#der_df = der_df.select(\"*\",substring(col(\"shipment_date\"),6,2).cast(\"int\").alias(\"shipment_month\"))\n",
    "der_df = der_df.withColumn(\"shipment_month\", month(\"shipment_date\"))\n",
    "#display(der_df)\n",
    "\n",
    "#3.Flag Weekend Operations (is_weekend)\n",
    "#Scenario: The Operations team needs to track shipments handled during weekends to calculate overtime pay or analyze non-business day capacity.\n",
    "#Action: Flag as 'True' if the shipment_date falls on a Saturday or Sunday.\n",
    "\n",
    "der_df = der_df.withColumn(\"is_weekend\", when(dayofweek(col(\"shipment_date\")).isin(1,7), True).otherwise(False))\n",
    "#display(der_df)\n",
    "\n",
    "#4.lag shipment status (is_expedited)\n",
    "#Scenario: The Operations team needs to track shipments is IN_TRANSIT or DELIVERED.\n",
    "#Action: Flag as 'True' if the shipment_status IN_TRANSIT or DELIVERED. \n",
    "\n",
    "der_df = der_df.withColumn(\"is_expedited\",when(col(\"shipment_status\").isin(\"IN_TRANSIT\",\"DELIVERED\"),True).otherwise(False))\n",
    "display(der_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f178441f-3675-448e-b8f1-f45336851f82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### Enrichment/Business Logics (Calculated Fields)\n",
    "*Deriving new metrics and financial indicators using mathematical and date-based operations.*<br>\n",
    "Source File: logistics_shipment_detail_3000.json<br>\n",
    "\n",
    "**1. Calculate Unit Cost (`cost_per_kg`)**\n",
    "* **Scenario:** The Finance team wants to analyze the efficiency of shipments by determining the cost incurred per unit of weight.\n",
    "* **Action:** Divide `shipment_cost` by `shipment_weight_kg`.\n",
    "* **Logic:** `shipment_cost / shipment_weight_kg`\n",
    "\n",
    "**2. Track Shipment Age (`days_since_shipment`)**\n",
    "* **Scenario:** The Operations team needs to monitor how long it has been since a shipment was dispatched to identify potential delays.\n",
    "* **Action:** Calculate the difference in days between the `current_date` and the `shipment_date`.\n",
    "* **Logic:** `datediff(current_date(), shipment_date)`\n",
    "\n",
    "**3. Compute Tax Liability (`tax_amount`)**\n",
    "* **Scenario:** For invoicing and compliance, we must calculate the Goods and Services Tax (GST) applicable to each shipment.\n",
    "* **Action:** Calculate 18% GST on the total `shipment_cost`.\n",
    "* **Logic:** `shipment_cost * 0.18`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1725ca72-9149-499c-ad2d-0256b93fb0a3",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768738027273}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1. Calculate Unit Cost (cost_per_kg)\n",
    "\n",
    "#enr_df=der_df.withColumn(\"cost_per_kg\",col(\"shipment_cost\")/col(\"shipment_weight_kg\"))\n",
    "enr_df = der_df.withColumn( \"cost_per_kg\",when(col(\"shipment_weight_kg\") > 0,round(col(\"shipment_cost\") / col(\"shipment_weight_kg\"), 2)\n",
    ").otherwise(None))\n",
    "#display(enr_df)\n",
    "\n",
    "#2.Track Shipment Age (days_since_shipment)\n",
    "#enr_df = enr_df.withColumn(\"days_since_shipment\",datediff(col(\"ingestion_timestamp\"),col(\"shipment_date\")))\n",
    "enr_df = enr_df.withColumn(\"days_since_shipment\",datediff(current_date(),col(\"shipment_date\")))\n",
    "#display(enr_df)\n",
    "\n",
    "\n",
    "#3.Compute Tax Liability (tax_amount)\n",
    "enr_df = enr_df.withColumn(\"tax_amount\",round(col(\"shipment_cost\")*0.18,2))\n",
    "#display(enr_df)\n",
    "display(enr_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59f1af79-9d38-4799-a562-6bca93ad539b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Grouping & Aggregations (Advanced) - Fixed"
    }
   },
   "outputs": [],
   "source": [
    "rawdf2= spark.read.csv(\"/Volumes/logistics_project/default/logistics/logistics_source2\",header=True,inferSchema=False)\r\n",
    "rawdf2 = rawdf1.unionByName(rawdf2, allowMissingColumns=True).where(\"shipment_id != 'ten'\")\r\n",
    "#display(rawdf2)\r\n",
    "#display(mask_df)\r\n",
    "rawdf3 = mask_df.withColumnRenamed(\"vehicle_type\", \"lvehicle_type\").where(\"shipment_id != 'ten'\")\r\n",
    "#display(rawdf3)\r\n",
    "flatdf = rawdf3.join(std_json_df,how = \"left\", on = \"shipment_id\")\r\n",
    "#display(flatdf)\r\n",
    "grpdf = flatdf.groupBy(\"origin_hub_city\",\"lvehicle_type\").agg(sum(\"shipment_cost\").alias(\"Overallshipment_cost\")).orderBy(\"origin_hub_city\",\"lvehicle_type\")\r\n",
    "display(grpdf)\r\n",
    "\r\n",
    "#Scenario: The CFO wants a subtotal report at multiple levels:\r\n",
    "#Total Cost by Hub.\r\n",
    "totdf = flatdf.rollup(\"origin_hub_city\").agg(sum(\"shipment_cost\")).orderBy(\"origin_hub_city\",ascending = [False]).alias(\"rollup_cost\")\r\n",
    "#display(totdf)\r\n",
    "#Total Cost by Hub AND Vehicle Type.\r\n",
    "#totdf = flatdf.rollup(\"origin_hub_city\",\"lvehicle_type\").agg(sum(\"shipment_cost\").alias(\"rollup_cost\")).orderBy(\"origin_hub_city\",\"rollup_cost\", ascending = [False,False]).alias(\"rollup_cost\")\r\n",
    "#display(totdf)\r\n",
    "#Grand Total.\r\n",
    "#totdf = flatdf.agg(sum(\"shipment_cost\"))\r\n",
    "#display(totdf)\r\n",
    "#Action: Use cube(\"origin_hub_city\", \"lvehicle_type\") or rollup() to generate all these subtotals in a single query.\r\n",
    "#cbdf = flatdf.cube(\"origin_hub_city\",\"lvehicle_type\").agg(sum(\"shipment_cost\").alias(\"rollup_cost\")).orderBy(\"origin_hub_city\",\"rollup_cost\",ascending = [False,False]).alias(\"rollup_cost\")\r\n",
    "#display(cbdf)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9de85d4f-d903-46fd-b110-1476a2383d6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### Remove/Eliminate (drop, select, selectExpr)\n",
    "*Excluding unnecessary or redundant columns to optimize storage and privacy.*<br>\n",
    "Source File: DF of logistics_source1 and logistics_source2<br>\n",
    "\n",
    "**1. Remove Redundant Name Columns**\n",
    "* **Scenario:** Since we have already created the `full_name` column in the Enrichment step, the individual name columns are now redundant and clutter the dataset.\n",
    "* **Action:** Drop the `first_name` and `last_name` columns.\n",
    "* **Logic:** `df.drop(\"first_name\", \"last_name\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd95ec98-ce5a-4160-94bb-d5857ad5a378",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#Remove Redundant Name Columns\n",
    "std_csv_df.printSchema()\n",
    "csv_df =std_csv_df.drop(\"staff_first_name\",\"staff_last_name\")\n",
    "csv_df = csv_df.select(\"shipment_id\",\"full_name\",\"role\",\"age\",\"vehicle_type\",\"origin_hub_city\",\"load_dt\")\n",
    "display(csv_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7682d4f7-a188-4f86-b60f-c1afa5db220f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Splitting & Merging/Melting of Columns\n",
    "*Reshaping columns to extract hidden values or combine fields for better analysis.*<br>\n",
    "Source File: DF of logistics_shipment_detail_3000.json<br>\n",
    "**1. Splitting (Extraction)**\n",
    "*Breaking one column into multiple to isolate key information.*\n",
    "* **Split Order Code:**\n",
    "  * **Action:** Split `order_id` (\"ORD100000\") into two new columns:\n",
    "    * `order_prefix` (\"ORD\")\n",
    "    * `order_sequence` (\"100000\")\n",
    "* **Split Date:**\n",
    "  * **Action:** Split `shipment_date` into three separate columns for partitioning:\n",
    "    * `ship_year` (2024)\n",
    "    * `ship_month` (4)\n",
    "    * `ship_day` (23)\n",
    "\n",
    "**2. Merging (Concatenation)**\n",
    "*Combining multiple columns into a single unique identifier or description.*\n",
    "* **Create Route ID:**\n",
    "  * **Action:** Merge `source_city` (\"Chennai\") and `destination_city` (\"Pune\") to create a descriptive route key:\n",
    "    * `route_lane` (\"Chennai->Pune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6e587f8-b309-41bc-aab5-c398b484922f",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768740467009}",
       "filterBlob": "{\"version\":1,\"filterGroups\":[],\"syncTimestamp\":1768747155584}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1.Split Order Code:\n",
    "#Action: Split order_id (\"ORD100000\") into two new columns:\n",
    "#order_prefix (\"ORD\")\n",
    "#order_sequence (\"100000\")\n",
    "\n",
    "split_df = enr_df \\\n",
    "    .withColumn(\"order_prefix\", regexp_extract(col(\"order_id\"), \"([A-Za-z]+)\", 1)) \\\n",
    "    .withColumn(\"order_sequence\", regexp_extract(col(\"order_id\"), \"(\\\\d+)\", 1))\n",
    "#display(split_df)\n",
    "\n",
    "#2.Split shipment_date into three separate columns for partitioning:\n",
    "\n",
    "split_df = split_df.withColumn(\"ship_year\", substring(col(\"shipment_date\"), 1, 4)).withColumn(\"ship_month\", substring(col(\"shipment_date\"), 6, 2)).withColumn(\"ship_day\", substring(col(\"shipment_date\"), 9, 2))\n",
    "#display(split_df)\n",
    "\n",
    "\n",
    "#2.Merging\n",
    "# Action: Merge source_city (\"Chennai\") and destination_city (\"Pune\") to create a descriptive route key\n",
    "\n",
    "merge_df = split_df.withColumn(\"route_lane\",concat(col(\"source_city\"),lit(\"->\"),col(\"destination_city\")))\n",
    "display(merge_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3a4891e-0c24-40b4-8ed8-b124efed02f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Data Customization & Processing - Application of Tailored Business Specific Rules\n",
    "\n",
    "### **UDF1: Complex Incentive Calculation**\n",
    "**Scenario:** The Logistics Head wants to calculate a \"Performance Bonus\" for drivers based on tenure and role complexity.\n",
    "\n",
    "**Action:** Create a Python function `calculate_bonus(role, age)` and register it as a Spark UDF.\n",
    "\n",
    "**Logic:**\n",
    "* **IF** `Role` == 'Driver' **AND** `Age` > 50:\n",
    "  * `Bonus` = 15% of Salary (Reward for Seniority)\n",
    "* **IF** `Role` == 'Driver' **AND** `Age` < 30:\n",
    "  * `Bonus` = 5% of Salary (Encouragement for Juniors)\n",
    "* **ELSE**:\n",
    "  * `Bonus` = 0\n",
    "\n",
    "**Result:** A new derived column `projected_bonus` is generated for every row in the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### **UDF2: PII Masking (Privacy Compliance)**\n",
    "**Scenario:** For the analytics dashboard, we must hide the full identity of the staff to comply with privacy laws (GDPR/DPDP), while keeping names recognizable for internal managers.\n",
    "\n",
    "**Business Rule:** Show the first 2 letters, mask the middle characters with `****`, and show the last letter.\n",
    "\n",
    "**Action:** Create a UDF `mask_identity(name)`.\n",
    "\n",
    "**Example:**\n",
    "* **Input:** `\"Rajesh\"`\n",
    "* **Output:** `\"Ra****h\"`\n",
    "<br>\n",
    "**Note: Convert the above udf logic to inbult function based transformation to ensure the performance is improved.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d398acdb-13dc-4650-9d4e-0d1995717630",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768745192886}",
       "filterBlob": "{\"version\":1,\"filterGroups\":[],\"syncTimestamp\":1768745318645}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1.UDF1: Complex Incentive Calculation\n",
    "\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# UDF Logic\n",
    "def calculate_bonus(role, age):\n",
    "    if role == 'Driver' and age is not None and age > 50:\n",
    "        return 15\n",
    "    elif role == 'Driver' and age is not None and age < 30:\n",
    "        return 5\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Register UDF with return type\n",
    "calculate_bonus_udf = udf(calculate_bonus, IntegerType())\n",
    "\n",
    "# Apply UDF (DO NOT cast age to string)\n",
    "bonus_df = csv_df.withColumn(\"projected_bonus\",calculate_bonus_udf(col(\"role\"), col(\"age\").cast(\"int\")))\n",
    "display(bonus_df)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c219197c-9d8b-47ae-a7c5-ae7c98c11b69",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768746164734}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#DF2: PII Masking (Privacy Compliance)\n",
    "#Create a UDF mask_identity(name)\n",
    "#Input: \"Rajesh\"\n",
    "#Output: \"Ra****h\"\n",
    "#**Note: Convert the above udf logic to inbult function based transformation to ensure the performance is improved.**\n",
    "\n",
    "def mask_identity(name):\n",
    "    if name is None or len(name) <=3:\n",
    "        return name\n",
    "    else:\n",
    "        return name[:2]+ \"*****\" + name[-1]\n",
    "    \n",
    "masked_udf = udf(mask_identity)\n",
    "#mask_df = rawdf_merged.withColumn(\"mask_name\",masked_udf(col(\"first_name\")))\n",
    "mask_df = bonus_df.withColumn(\"mask_last_name\",masked_udf(col(\"full_name\")))\n",
    "display(mask_df)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ed3d160-f54e-4bf3-bb2c-40ae7ff1b7d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Data Core Curation & Processing (Pre-Wrangling)\n",
    "*Applying business logic to focus, filter, and summarize data before final analysis.*\n",
    "\n",
    "**1. Select (Projection)**<br>\n",
    "Source Files: DF of logistics_source1 and logistics_source2<br>\n",
    "* **Scenario:** The Driver App team only needs location data, not sensitive HR info.\n",
    "* **Action:** Select only `first_name`, `role`, and `hub_location`.\n",
    "\n",
    "**2. Filter (Selection)**<br>\n",
    "Source File: DF of json<br>\n",
    "* **Scenario:** We need a report on active operational problems.\n",
    "* **Action:** Filter rows where `shipment_status` is **'DELAYED'** or **'RETURNED'**.\n",
    "* **Scenario:** Insurance audit for senior staff.\n",
    "* **Action:** Filter rows where `age > 50`.\n",
    "\n",
    "**3. Derive Flags & Columns (Business Logic)**<br>\n",
    "Source File: DF of json<br>\n",
    "* **Scenario:** Identify high-value shipments for security tracking.\n",
    "* **Action:** Create flag `is_high_value` = **True** if `shipment_cost > 50,000`.\n",
    "* **Scenario:** Flag weekend operations for overtime calculation.\n",
    "* **Action:** Create flag `is_weekend` = **True** if day is Saturday or Sunday.\n",
    "\n",
    "**4. Format (Standardization)**<br>\n",
    "Source File: DF of json<br>\n",
    "* **Scenario:** Finance requires readable currency formats.\n",
    "* **Action:** Format `shipment_cost` to string like **\"₹30,695.80\"**.\n",
    "* **Scenario:** Standardize city names for reporting.\n",
    "* **Action:** Format `source_city` to Uppercase (e.g., \"chennai\" → **\"CHENNAI\"**).\n",
    "\n",
    "**5. Group & Aggregate (Summarization)**<br>\n",
    "Source Files: DF of logistics_source1 and logistics_source2<br>\n",
    "* **Scenario:** Regional staffing analysis.\n",
    "* **Action:** Group by `hub_location` and **Count** the number of staff.\n",
    "* **Scenario:** Fleet capacity analysis.\n",
    "* **Action:** Group by `vehicle_type` and **Sum** the `shipment_weight_kg`.\n",
    "\n",
    "**6. Sorting (Ordering)**<br>\n",
    "Source File: DF of json<br>\n",
    "* **Scenario:** Prioritize the most expensive shipments.\n",
    "* **Action:** Sort by `shipment_cost` in **Descending** order.\n",
    "* **Scenario:** Organize daily dispatch schedule.\n",
    "* **Action:** Sort by `shipment_date` (Ascending).\n",
    "\n",
    "**7. Limit (Top-N Analysis)**<br>\n",
    "Source File: DF of json<br>\n",
    "* **Scenario:** Dashboard snapshot of critical delays.\n",
    "* **Action:** Filter for 'DELAYED', Sort by Cost, and **Limit to top 10** rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b08354fe-c342-45fb-ab05-a00bb6e09072",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768750656494}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Untitled"
    }
   },
   "outputs": [],
   "source": [
    "#1.Select (Projection) - Select only first_name, role, and hub_location.\n",
    "select_df = rawdf_merged.select(\"first_name\",\"role\",\"hub_location\")\n",
    "#display(select_df)\n",
    "\n",
    "#2.Filter (Selection)\n",
    " #Filter rows where shipment_status is 'DELAYED' or 'RETURNED'.\n",
    "\n",
    "merge_df = merge_df.filter((col(\"shipment_status\") ==\"DELAYED\" )| (col(\"shipment_status\") ==\"RETURNED\"))\n",
    "#display(merge_df)\n",
    "\n",
    "# Filter rows where age > 50\n",
    "\n",
    "filter_df =csv_df.filter(col(\"age\") > 50)\n",
    "#display(filter_df)\n",
    "\n",
    "#3.3. Derive Flags & Columns (Business Logic)\n",
    "#Source File: DF of json\n",
    "#Scenario: Identify high-value shipments for security tracking.\n",
    "#Action: Create flag is_high_value = True if shipment_cost > 50,000.\n",
    "#Scenario: Flag weekend operations for overtime calculation.\n",
    "#Action: Create flag is_weekend = True if day is Saturday or Sunday.\n",
    "\n",
    "flag_df = merge_df.withColumn(\"is_high_value\",when(col(\"shipment_cost\") > 40000, lit(True)).otherwise(lit(False))).withColumn(\"is_weekend\",when(dayofweek(col(\"shipment_date\")).isin([1,7]), lit(True)).otherwise(lit(False)))\n",
    "#display(flag_df)\n",
    "\n",
    "\n",
    "#4.Format (Standardization)\n",
    "# Format shipment_cost to string like \"₹30,695.80\"\n",
    "\n",
    "format_df=merge_df.withColumn(\"shipment_cost\",concat(lit(\"₹\"),format_number(col(\"shipment_cost\"),2)))\n",
    "#display(format_df)\n",
    "\n",
    "#Format source_city to Uppercase (e.g., \"chennai\" → \"CHENNAI\").\n",
    "format_df = format_df.withColumn(\"source_city\",upper(col(\"source_city\")))\n",
    "#display(format_df)\n",
    "\n",
    "#5 Group & Aggregate (Summarization)\n",
    "\n",
    "#Group by hub_location and Count the number of staff.\n",
    "group_df =mask_df.groupBy(\"origin_hub_city\").agg(count(\"shipment_id\").alias(\"staff_count\"))\n",
    "#display(group_df)\n",
    "\n",
    "#Group by vehicle_type and Sum the shipment_weight_kg.\n",
    "\n",
    "grp_json_df = format_df.groupBy(\"vehicle_type\").agg(round(sum(\"shipment_weight_kg\"),2))\n",
    "#display(grp_json_df)\n",
    "\n",
    "\n",
    "#6. Sorting (Ordering)\n",
    "\n",
    "# Sort by shipment_cost in Descending order.\n",
    "sort_df = flag_df.orderBy(col(\"shipment_cost\").desc())\n",
    "#display(sort_df)\n",
    "#Scenario: Organize daily dispatch schedule.\n",
    "#Action: Sort by shipment_date (Ascending).\n",
    "\n",
    "sort_df= sort_df.orderBy(col(\"shipment_date\").asc())\n",
    "#display(sort_df)\n",
    "\n",
    "#7.Limit (Top-N Analysis)\n",
    "#Scenario: Dashboard snapshot of critical delays.\n",
    "#Action: Filter for 'DELAYED', Sort by Cost, and Limit to top 10 rows.\n",
    "\n",
    "limit_df =flag_df.filter(col(\"shipment_status\")==\"DELAYED\").orderBy(col(\"shipment_cost\").desc())\n",
    "display(limit_df.limit(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9992924-2d11-4cfa-b8fa-5c96bf6d0475",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Data Wrangling - Transformation & Analytics\n",
    "*Combining, modeling, and analyzing data to answer complex business questions.*\n",
    "\n",
    "### **1. Joins**\n",
    "Source Files:<br>\n",
    "Left Side (staff_df):<br> DF of logistics_source1 & logistics_source2<br>\n",
    "Right Side (shipments_df):<br> DF of logistics_shipment_detail_3000.json<br>\n",
    "#### **1.1 Frequently Used Simple Joins (Inner, Left)**\n",
    "* **Inner Join (Performance Analysis):**\n",
    "  * **Scenario:** We only want to analyze *completed work*. Connect Staff to the Shipments they handled.\n",
    "  * **Action:** Join `staff_df` and `shipments_df` on `shipment_id`.\n",
    "  * **Result:** Returns only rows where a staff member is assigned to a valid shipment.\n",
    "* **Left Join (Idle Resource check):**\n",
    "  * **Scenario:** Find out which staff members are currently *idle* (not assigned to any shipment).\n",
    "  * **Action:** Join `staff_df` (Left) with `shipments_df` (Right) on `shipment_id`. Filter where `shipments_df.shipment_id` is NULL.\n",
    "\n",
    "#### **1.2 Infrequent Simple Joins (Self, Right, Full, Cartesian)**\n",
    "* **Self Join (Peer Finding):**\n",
    "  * **Scenario:** Find all pairs of employees working in the same `hub_location`.\n",
    "  * **Action:** Join `staff_df` to itself on `hub_location`, filtering where `staff_id_A != staff_id_B`.\n",
    "* **Right Join (Orphan Data Check):**\n",
    "  * **Scenario:** Identify shipments in the system that have *no valid driver* assigned (Data Integrity Issue).\n",
    "  * **Action:** Join `staff_df` (Left) with `shipments_df` (Right). Focus on NULLs on the left side.\n",
    "* **Full Outer Join (Reconciliation):**\n",
    "  * **Scenario:** A complete audit to find *both* idle drivers AND unassigned shipments in one view.\n",
    "  * **Action:** Perform a Full Outer Join on `shipment_id`.\n",
    "* **Cartesian/Cross Join (Capacity Planning):**\n",
    "  * **Scenario:** Generate a schedule of *every possible* driver assignment to *every* pending shipment to run an optimization algorithm.\n",
    "  * **Action:** Cross Join `drivers_df` and `pending_shipments_df`.\n",
    "\n",
    "#### **1.3 Advanced Joins (Semi and Anti)**\n",
    "* **Left Semi Join (Existence Check):**\n",
    "  * **Scenario:** \"Show me the details of Drivers who have *at least one* shipment.\" (Standard filtering).\n",
    "  * **Action:** `staff_df.join(shipments_df, \"shipment_id\", \"left_semi\")`.\n",
    "  * **Benefit:** Performance optimization; it stops scanning the right table once a match is found.\n",
    "* **Left Anti Join (Negation Check):**\n",
    "  * **Scenario:** \"Show me the details of Drivers who have *never* touched a shipment.\"\n",
    "  * **Action:** `staff_df.join(shipments_df, \"shipment_id\", \"left_anti\")`.\n",
    "\n",
    "### **2. Lookup**<br>\n",
    "Source File: DF of logistics_source1 and logistics_source2 (merged into Staff DF) and Master_City_List.csv<br>\n",
    "* **Scenario:** Validation. Check if the `hub_location` in the staff file exists in the dataframe of corporate `Master_City_List.csv`.\n",
    "* **Action:** Compare values against this Master_City_List list.\n",
    "\n",
    "### **3. Lookup & Enrichment**<br>\n",
    "Source File: DF of logistics_source1 and logistics_source2 (merged into Staff DF) and Master_City_List.csv dataframe<br>\n",
    "* **Scenario:** Geo-Tagging.\n",
    "* **Action:** Lookup `hub_location` (eg. \"Pune\") in a Master Latitude/Longitude Master_City_List.csv dataframe and enrich our logistics_source (merged dataframe) by adding `lat` and `long` columns for map plotting.\n",
    "\n",
    "### **4. Schema Modeling (Denormalization)**<br>\n",
    "Source Files: DF of All 3 Files (logistics_source1, logistics_source2, logistics_shipment_detail_3000.json)<br>\n",
    "* **Scenario:** Creating a \"Gold Layer\" Table for PowerBI/Tableau.\n",
    "* **Action:** Flatten the Star Schema. Join `Staff`, `Shipments`, and `Vehicle_Master` into one wide table (`wide_shipment_history`) so analysts don't have to perform joins during reporting.\n",
    "\n",
    "### **5. Windowing (Ranking & Trends)**<br>\n",
    "Source Files:<br>\n",
    "DF of logistics_source2: Provides hub_location (Partition Key).<br>\n",
    "logistics_shipment_detail_3000.json: Provides shipment_cost (Ordering Key)<br>\n",
    "* **Scenario:** \"Who are the Top 3 Drivers by Cost in *each* Hub?\"\n",
    "* **Action:**\n",
    "  1. Partition by `hub_location`.\n",
    "  2. Order by `total_shipment_cost` Descending.\n",
    "  3. Apply `dense_rank()` and `row_number()\n",
    "  4. Filter where `rank or row_number <= 3`.\n",
    "\n",
    "### **6. Analytical Functions (Lead/Lag)**<br>\n",
    "Source File: <br>\n",
    "DF of logistics_shipment_detail_3000.json<br>\n",
    "* **Scenario:** Idle Time Analysis.\n",
    "* **Action:** For each driver, calculate the days elapsed since their *previous* shipment.\n",
    "\n",
    "### **7. Set Operations**<br>\n",
    "Source Files: DF of logistics_source1 and logistics_source2<br>\n",
    "* **Union:** Combining `Source1` (Legacy) and `Source2` (Modern) into one dataset (Already done in Active Munging).\n",
    "* **Intersect:** Identifying Staff IDs that appear in *both* Source 1 and Source 2 (Duplicate/Migration Check).\n",
    "* **Except (Difference):** Identifying Staff IDs present in Source 2 but *missing* from Source 1 (New Hires).\n",
    "\n",
    "### **8. Grouping & Aggregations (Advanced)**<br>\n",
    "Source Files:<br>\n",
    "DF of logistics_source2: Provides hub_location and vehicle_type (Grouping Dimensions).<br>\n",
    "DF of logistics_shipment_detail_3000.json: Provides shipment_cost (Aggregation Metric).<br>\n",
    "* **Scenario:** The CFO wants a subtotal report at multiple levels:\n",
    "  1. Total Cost by Hub.\n",
    "  2. Total Cost by Hub AND Vehicle Type.\n",
    "  3. Grand Total.\n",
    "* **Action:** Use `cube(\"hub_location\", \"vehicle_type\")` or `rollup()` to generate all these subtotals in a single query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9eb5bc7f-4299-44b3-a224-17584c22e27f",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769415235364}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Joins (fixed CAST_INVALID_INPUT error)"
    }
   },
   "outputs": [],
   "source": [
    "rawdf_merged = rawdf_merged.filter(col(\"shipment_id\").rlike(\"^[0-9]+$\"))\n",
    "#display(rawdf_merged)\n",
    "#1. Joins\n",
    "staff_df = rawdf_merged\n",
    "shipments_df = std_json_df\n",
    "\n",
    "#1.1 Frequently Used Simple Joins (Inner, Left)\n",
    "#Inner Join (Performance Analysis):\n",
    "##Scenario: We only want to analyze completed work. Connect Staff to the Shipments they handled.\n",
    "#Action: Join staff_df and shipments_df on shipment_id.\n",
    "#Result: Returns only rows where a staff member is assigned to a valid shipment.\n",
    "\n",
    "inner_join_df = staff_df.join(shipments_df, how=\"inner\", on=\"shipment_id\")\n",
    "#display(inner_join_df)\n",
    "\n",
    "#Left Join (Idle Resource check):\n",
    "#Scenario: Find out which staff members are currently idle (not assigned to any shipment).\n",
    "#Action: Join staff_df (Left) with shipments_df (Right) on shipment_id. Filter where shipments_df.shipment_id is NULL.\n",
    "\n",
    "left_join_df = staff_df.join(shipments_df, how=\"left\", on=\"shipment_id\").filter(shipments_df.shipment_id.isNull())\n",
    "#display(left_join_df)\n",
    "\n",
    "\n",
    "#1.2 Infrequent Simple Joins (Self, Right, Full, Cartesian)\n",
    "#Self Join (Peer Finding):\n",
    "#Scenario: Find all pairs of employees working in the same hub_location.\n",
    "#Action: Join staff_df to itself on hub_location, filtering where staff_id_A != staff_id_B.\n",
    "\n",
    "a = staff_df.withColumnRenamed(\"shipment_id\", \"shipment_id_A\")\n",
    "b = staff_df.withColumnRenamed(\"shipment_id\", \"shipment_id_B\")\n",
    "c = a.join(b, how = \"inner\", on = \"hub_location\").filter(a[\"shipment_id_A\"] != b[\"shipment_id_B\"])\n",
    "#display(c)\n",
    "\n",
    "#Right Join (Orphan Data Check):\n",
    "#Scenario: Identify shipments in the system that have no valid driver assigned (Data Integrity Issue).\n",
    "#Action: Join staff_df (Left) with shipments_df (Right). Focus on NULLs on the left side.\n",
    "\n",
    "staff_df = staff_df.withColumn(\"shipment_ky\",col(\"shipment_id\"))\n",
    "rgtdf = staff_df.join(shipment_df,how = \"right\", on = \"shipment_id\").filter(col(\"shipment_ky\").isNull())\n",
    "#display(rgtdf)\n",
    "\n",
    "#Full Outer Join (Reconciliation):\n",
    "#Scenario: A complete audit to find both idle drivers AND unassigned shipments in one view.\n",
    "#Action: Perform a Full Outer Join on shipment_id.\n",
    "\n",
    "full_df = staff_df.join(shipment_df,how =\"full\",on=\"shipment_id\")\n",
    "#display(full_df)\n",
    "\n",
    "#1.3 Advanced Joins (Semi, Anti)\n",
    "\n",
    "\n",
    "#Cartesian/Cross Join (Capacity Planning):\n",
    "#Scenario: Generate a schedule of every possible driver assignment to every pending shipment to run an optimization algorithm.\n",
    "#Action: Cross Join drivers_df and pending_shipments_df.\n",
    "\n",
    "cart_df = staff_df.join(shipment_df)\n",
    "#display(cart_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f96f3cb-e5cb-4782-bb64-e5c2e1330151",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1.3 Advanced Joins (Semi and Anti)\n",
    "#Left Semi Join (Existence Check):\n",
    "#Scenario: \"Show me the details of Drivers who have at least one shipment.\" (Standard filtering).\n",
    "#Action: staff_df.join(shipments_df, \"shipment_id\", \"left_semi\").\n",
    "#Benefit: Performance optimization; it stops scanning the right table once a match is found.\n",
    "semi_df = staff_df.join(shipment_df,how =\"left_semi\",on=\"shipment_id\")\n",
    "#display(semi_df)\n",
    "\n",
    "\n",
    "#Left Anti Join (Negation Check):\n",
    "#Scenario: \"Show me the details of Drivers who have never touched a shipment.\"\n",
    "#Action: staff_df.join(shipments_df, \"shipment_id\", \"left_anti\").\n",
    "\n",
    "anti_df = staff_df.join(shipment_df,how =\"left_anti\",on=\"shipment_id\")\n",
    "display(anti_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1c163f6-cc87-4701-8ec8-30082f37b484",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769416446931}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "geo_data = [\n",
    "    (\"AbuDhabi\", 24.4539, 54.3773),\n",
    "    (\"Ahmedabad\", 23.0225, 72.5714),\n",
    "    (\"Amritsar\", 31.6340, 74.8723),\n",
    "    (\"Bangalore\", 12.9716, 77.5946),\n",
    "    (\"Birmingham\", 52.4862, -1.8904),\n",
    "    (\"Boston\", 42.3601, -71.0589),\n",
    "    (\"California\", 36.7783, -119.4179),\n",
    "    (\"Chennai\", 13.0827, 80.2707),\n",
    "    (\"Chicago\", 41.8781, -87.6298),\n",
    "    (\"Coimbatore\", 11.0168, 76.9558),\n",
    "    (\"Delhi\", 28.7041, 77.1025),\n",
    "    (\"Dubai\", 25.2048, 55.2708),\n",
    "    (\"HongKong\", 22.3193, 114.1694),\n",
    "    (\"Hyderabad\", 17.3850, 78.4867),\n",
    "    (\"Indore\", 22.7196, 75.8577),\n",
    "    (\"Jaipur\", 26.9124, 75.7873),\n",
    "    (\"Kochi\", 9.9312, 76.2673),\n",
    "    (\"London\", 51.5074, -0.1278),\n",
    "    (\"Lucknow\", 26.8467, 80.9462),\n",
    "    (\"MexicoCity\", 19.4326, -99.1332),\n",
    "    (\"Mumbai\", 19.0760, 72.8777),\n",
    "    (\"NewYork\", 40.7128, -74.0060),\n",
    "    (\"Pune\", 18.5204, 73.8567),\n",
    "    (\"Scranton\", 41.4089, -75.6624),\n",
    "    (\"Singapore\", 1.3521, 103.8198),\n",
    "    (\"Texas\", 31.9686, -99.9018),\n",
    "    (\"Tokyo\", 35.6762, 139.6503)\n",
    "]\n",
    "\n",
    "geodf = spark.createDataFrame(geo_data, [\"hub_location\", \"latitude\", \"longitude\"])\n",
    "#display(geodf)\n",
    "\n",
    "#2. Lookup\n",
    "#Source File: DF of logistics_source1 and logistics_source2 (merged into Staff DF)\n",
    "#Scenario: Validation. Check if the hub_location in the staff file exists in the corporate Master_City_List.\n",
    "#Action: Compare values against a reference list.\n",
    "Master_City_List = rawdf2.select(\"hub_location\").distinct()\n",
    "#display(Master_City_List)\n",
    "lookupdf = Master_City_List.join(rawdf_merged, on = \"hub_location\", how = \"semi\")\n",
    "#display(lookupdf)\n",
    "\n",
    "#3. Lookup & Enrichment\n",
    "#Source File: DF of logistics_source1 and logistics_source2 (merged into Staff DF)\n",
    "#Scenario: Geo-Tagging.\n",
    "#Action: Lookup hub_location (\"Pune\") in a Master Latitude/Longitude table and enrich the dataset by adding lat and long columns for map plotting.\n",
    "lookupdf = rawdf_merged.join(geodf, on = \"hub_location\", how = \"inner\").filter(\"hub_location = 'Pune'\")\n",
    "#display(lookupdf)\n",
    "\n",
    "#4. Schema Modeling (Denormalization)\n",
    "#Source Files: DF of All 3 Files (logistics_source1, logistics_source2, logistics_shipment_detail_3000.json)\n",
    "#Scenario: Creating a \"Gold Layer\" Table for PowerBI/Tableau.\n",
    "#Action: Flatten the Star Schema. Join Staff, Shipments, and Vehicle_Master into one wide table (wide_shipment_history) so analysts don't have to perform joins during reporting.\n",
    "\n",
    "fltdf = rawdf_merged.join(std_json_df, on = \"shipment_id\", how = \"full\").filter(\"role = 'Driver'\")\n",
    "display(fltdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c46eebd2-3919-4f89-8a50-5e110f68aaca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import dense_rank,desc,row_number,lead,datediff,to_date,lag\n",
    "from pyspark.sql.window import Window\n",
    "#5. Windowing (Ranking & Trends)\n",
    "#Source Files:\n",
    "#DF of logistics_source2: Provides hub_location (Partition Key).\n",
    "#logistics_shipment_detail_3000.json: Provides shipment_cost (Ordering Key)\n",
    "#Scenario: \"Who are the Top 3 Drivers by Cost in each Hub?\"\n",
    "#Action:Partition by hub_location.\n",
    "# Order by total_shipment_cost Descending.\n",
    "#Apply dense_rank() and `row_number()\n",
    "#Filter where rank or row_number <= 3.\n",
    "\n",
    "winddf = fltdf.filter(\"role = 'Driver' and hub_location is not null\").withColumn(\"rn\",row_number().over(Window.partitionBy(\"hub_location\").orderBy(desc(\"shipment_cost\")))).\\\n",
    "    filter(\"rn <= 3\")\n",
    "\n",
    "#display(winddf)\n",
    "\n",
    "\n",
    "#6. Analytical Functions (Lead/Lag)\n",
    "#Source File:\n",
    "#DF of logistics_shipment_detail_3000.json\n",
    "#Scenario: Idle Time Analysis.\n",
    "#Action: For each driver, calculate the days elapsed since their previous shipment.\n",
    "leadlagdf = fltdf.filter(\"role = 'Driver'\").withColumn(\"days_since_last_shipment\", lag(\"shipment_date\", 1).over(Window.partitionBy(\"role\").orderBy(desc(\"shipment_date\")))).withColumn(\"shipment_date\",to_date(\"shipment_date\",\"dd-MM-yy\")).withColumn(\"days_since_last_shipment\",to_date(\"days_since_last_shipment\",\"dd-MM-yy\")).withColumn(\"days_elapsed\",datediff(\"shipment_date\",\"days_since_last_shipment\"))\n",
    "display(leadlagdf)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f451461-cedc-4842-88a4-32b99b244c5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#7. Set Operations\n",
    "#Source Files: DF of logistics_source1 and logistics_source2\n",
    "#Union: Combining Source1 (Legacy) and Source2 (Modern) into one dataset (Already done in Active Munging).\n",
    "#Intersect: Identifying Staff IDs that appear in both Source 1 and Source 2 (Duplicate/Migration Check).\n",
    "#Except (Difference): Identifying Staff IDs present in Source 2 but missing from Source 1 (New Hires).\n",
    "union_df = rawdf1.unionByName(rawdf2, allowMissingColumns=True)\n",
    "display(union_df)\n",
    "intersect_df = rawdf1.intersect(rawdf2)\n",
    "#display(intersectdf)\n",
    "exceptdf = rawdf1.exceptAll(rawdf2).dropDuplicates()\n",
    "#display(exceptdf)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2023303b-7a20-476a-96a6-efbfda9e89cf",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769419310016}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#8. Grouping & Aggregations (Advanced)\n",
    "#Source Files:\n",
    "#DF of logistics_source2: Provides hub_location and vehicle_type (Grouping Dimensions).\n",
    "#DF of logistics_shipment_detail_3000.json: Provides shipment_cost (Aggregation Metric).\n",
    "rawdf2= spark.read.csv(\"/Volumes/logistics_project/default/logistics/logistics_source2\",header=True,inferSchema=False)\n",
    "rawdf2 = rawdf1.unionByName(rawdf2, allowMissingColumns=True).where(\"shipment_id != 'ten'\")\n",
    "#display(rawdf2)\n",
    "#display(mask_df)\n",
    "rawdf3 = mask_df.withColumnRenamed(\"vehicle_type\", \"lvehicle_type\").where(\"shipment_id != 'ten'\")\n",
    "#display(rawdf3)\n",
    "flatdf = rawdf3.join(std_json_df,how = \"left\", on = \"shipment_id\")\n",
    "flatdf = flatdf.withColumnRenamed(\"origin_hub_city\", \"hub_location\")\n",
    "#display(flatdf)\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "grpdf = (\n",
    "    flatdf\n",
    "    .groupBy(\"hub_location\", \"lvehicle_type\")\n",
    "    .agg(sum(\"shipment_cost\").alias(\"overall_shipment_cost\"))\n",
    "    .orderBy(\"hub_location\", \"lvehicle_type\")\n",
    ")\n",
    "\n",
    "#display(grpdf)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Scenario: The CFO wants a subtotal report at multiple levels:\n",
    "#Total Cost by Hub.\n",
    "totdf = flatdf.rollup(\"hub_location\").agg(sum(\"shipment_cost\")).orderBy(\"hub_location\",ascending = [False]).alias(\"rollup_cost\")\n",
    "display(totdf)\n",
    "#Total Cost by Hub AND Vehicle Type.\n",
    "totdf = flatdf.rollup(\"hub_location\",\"lvehicle_type\").agg(sum(\"shipment_cost\").alias(\"rollup_cost\")).orderBy(\"hub_location\",\"rollup_cost\", ascending = [False,False]).alias(\"rollup_cost\")\n",
    "display(totdf)\n",
    "#Grand Total.\n",
    "totdf = flatdf.agg(sum(\"shipment_cost\"))\n",
    "display(totdf)\n",
    "#Action: Use cube(\"hub_location\", \"vehicle_type\") or rollup() to generate all these subtotals in a single query.\n",
    "cube_df = flatdf.cube(\"hub_location\",\"lvehicle_type\").agg(sum(\"shipment_cost\").alias(\"rollup_cost\")).orderBy(\"hub_location\",\"rollup_cost\",ascending = [False,False]).alias(\"rollup_cost\")\n",
    "display(cube_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fbdbf31-7a73-44f2-8e74-6c6fcf35d916",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##6. Data Persistance (LOAD)-> Data Publishing & Consumption<br>\n",
    "\n",
    "Store the inner joined, lookup and enrichment, Schema Modeling, windowing, analytical functions, set operations, grouping and aggregation data into the delta tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b88eda04-9978-45db-95be-6b73371e0085",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "flatdf.write.json(\"/Volumes/logistics_project/default/logistics/logistics_US_Json/\",mode = \"overwrite\")\n",
    "mask_df.write.csv(\"/Volumes/logistics_project/default/logistics/logistics_US_CSV/\",mode = \"overwrite\")\n",
    "cube_df.write.parquet(\"/Volumes/logistics_project/default/logistics/logistics_US_Parquet/\",mode = \"overwrite\")\n",
    "grpdf.write.format(\"delta\").save(\"/Volumes/logistics_project/default/logistics/logistics_US_delta/\",mode = \"overwrite\")\n",
    "mask_df.write.mode(\"overwrite\").saveAsTable(\"logistics_US\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aee7e8d0-1fbb-4a6c-af39-316374a94a67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##7.Take the copy of the above notebook and try to write the equivalent SQL for which ever applicable."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "BB2_Usecase2_DSL_SQL_Paartha",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
